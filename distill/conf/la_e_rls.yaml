hydra:
  run:
    dir: outputs/la_e_rls/${now:%Y-%m-%d}/${now:%H-%M-%S}

wandb:
  project: "one-pass-distill"
  entity: null
  name: "LA-E-RLS_${now:%Y-%m-%d_%H-%M-%S}"
  mode: "online"

data:
  batch_size: 64
  num_workers: 24
  dataset_name: cifar100
  use_augmentation: true  # Toggle RandomCrop + RandomHorizontalFlip

model:
  name: resnet20
  num_classes: 100

train:
  epochs: 240
  lr: 0.05
  momentum: 0.9
  weight_decay: 0.0005

# ==============================================================================
# LA-E-RLS: Loss-Adaptive Entropy-controlled Randomized Label Smoothing
# ==============================================================================
# All core parameters are PRINCIPLED and derived from C (num_classes).
# Only override if you know what you're doing.
# ==============================================================================

la_e_rls:
  # ---------------------------------------------------------------------------
  # Effective Number of Confused Classes (k)
  # ---------------------------------------------------------------------------
  # k_min: Easy samples confuse between ~k_min classes
  # k_max: Hard samples confuse between ~k_max classes
  # Default: k_min=2, k_max=sqrt(C)  (Dataset-agnostic)
  k_min: 2
  k_max: 0  # 0 = auto (sqrt(C))
  
  # ---------------------------------------------------------------------------
  # Scaling Constant (c)
  # ---------------------------------------------------------------------------
  # From label smoothing: alpha=0.1 at k=2 → c = alpha * k = 0.2
  # Alpha range: [c/k_max, c/k_min] = [c/sqrt(C), c/2]
  c: 0.2
  
  # ---------------------------------------------------------------------------
  # Temperature Search
  # ---------------------------------------------------------------------------
  tau_min: 0.5   # Minimum temperature (sharpest)
  tau_max: 5.0   # Maximum temperature (flattest)
  tau_kd: 4.0    # Distillation temperature for KL loss scaling
  bs_iters: 15   # Binary search iterations for tau*
  
  # ---------------------------------------------------------------------------
  # Lambda Schedule
  # ---------------------------------------------------------------------------
  # cosine: Cosine increasing from 0 to lambda_val (default)
  # static: Fixed at lambda_val from epoch 0
  # adaptive: Scaled by batch mastery (experimental)
  lambda_schedule: "cosine"
  lambda_val: 0.5  # Maximum lambda (weight of KL loss)
  
  # ---------------------------------------------------------------------------
  # Sorting Mode
  # ---------------------------------------------------------------------------
  # fully_sorted: Sort both teacher and student distributions (shape matching)
  # gt_anchored: Keep GT position fixed, sort only non-GT classes
  sorting_mode: "fully_sorted"
  
  # ---------------------------------------------------------------------------
  # Misc
  # ---------------------------------------------------------------------------
  uniform_mix: 0.0  # Uniform floor for long-tail (0.05 → 5% uniform mix)
  seed: 42
  dry_run: false